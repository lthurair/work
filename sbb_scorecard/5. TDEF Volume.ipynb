{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TDEF.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import HiveContext, SQLContext,SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date,timedelta\n",
    "from datetime import datetime\n",
    "sc = SparkContext(appName = \"Combined_Table\")\n",
    "sql_context = HiveContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "spark = (SparkSession.builder.enableHiveSupport().getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from numpy import nan\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('/home/sharb24/SBB/thural2/record.py').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## READ THE TDEF TABLE ###############################\n",
    "df2 = spark.table('anp_cabbtdct1_working.SBB_TDEF')\n",
    "df_tdef=df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tdef.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tdef.columns=df_tdef.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### CHANGE DATE FORMAT ###########################\n",
    "from datetime import datetime\n",
    "\n",
    "########## get numeric month\n",
    "df_tdef['month_dt'] = pd.to_datetime(df_tdef.fundingdate, format='%m/%d/%Y')\n",
    "df_tdef['month_dt'] = df_tdef['month_dt'].dt.month\n",
    "\n",
    "import calendar\n",
    "df_tdef['month_dt'] = df_tdef['month_dt'].apply(lambda x: calendar.month_name[x]).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tdef.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### PROCESS TDEF DATA #############################\n",
    "\n",
    "## convert amountfunded to float\n",
    "df_tdef = df_tdef.astype({'amountfunded': float})\n",
    "\n",
    "## create a variable to identify the metric\n",
    "df_tdef1=df_tdef.assign(metric_name='TDEF Volume')\n",
    "df_tdef2=df_tdef1[['metric_name','asmbloginid','month_dt','amountfunded']]\n",
    "\n",
    "##rename 'asmbloginid' to acf2_id\n",
    "df_tdef2.rename(columns = {\"asmbloginid\":'acf2_id'}, inplace = True) \n",
    "\n",
    "## group by logon_id, metric_name, month\n",
    "df_tdef3=df_tdef2.groupby(['acf2_id','month_dt','metric_name'],as_index=False).aggregate({\n",
    "                'amountfunded':'sum'  # count widgets\n",
    "                })\n",
    "\n",
    "##transpose data\n",
    "df_tdef4 = df_tdef3.pivot_table(index=['acf2_id','metric_name'], columns=['month_dt'],\n",
    "                     values='amountfunded', aggfunc='first', fill_value=0).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get months\n",
    "##rename columns **************CHANGE FOR FUTURE\n",
    "cols = ['november','december','january','february','march','april','may','june','july',\n",
    "        'august','september','october']\n",
    "net_a = pd.DataFrame(list, index=['a',], columns=cols)\n",
    "\n",
    "################## CHANGE EVERY FY #############################\n",
    "\n",
    "complementary = [c for c in net_a if c not in df_tdef4]\n",
    "df_tdef5= pd.concat([net_a[complementary], df_tdef4], axis=1, join='outer')\n",
    "\n",
    "##delete the extra filler row\n",
    "df_tdef5= df_tdef5.drop(labels='a', axis=0)\n",
    "##convert to float\n",
    "df_tdef5 = df_tdef5.astype({'november': float,'december': float,\n",
    "                                'january': float, 'february': float, 'march': float,\n",
    "                                'april': float, 'may': float,'june':float,'july': float,\n",
    "                                'august': float, 'september':float,'october': float})\n",
    "\n",
    "##order the vars\n",
    "df_tdef5=df_tdef5[['metric_name','acf2_id','november','december','january',\n",
    "           'february','march','april','may','june','july','august','september','october']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### PROCESS FOR AMSB #############################################\n",
    "newvarfunc(df_tdef5)\n",
    "\n",
    "############## merge to hr/alignment to get am/sm cc\n",
    "df=spark.sql(\"select acf2_id,employee_name,am_cost_center,am_cost_center_name,am_cost_center_full_name, \\\n",
    "                     sm_cost_center,sm_cost_center_name from \\\n",
    "                     (select *, row_number() over(partition by acf2_id order by record_date desc) as ran \\\n",
    "                     from anp_cabbtdct1_final.HRM_SBB_ALIGNMENT0)b where ran=1\")\n",
    "dfhr=df.toPandas()\n",
    "\n",
    "## merge \n",
    "df_tdef55=pd.merge(df_tdef5,dfhr,how='left',left_on=['acf2_id'],right_on=['acf2_id'])\n",
    "\n",
    "## replace 0 with NaN so median only counts non-zeros\n",
    "df_tdef55.replace(0, np.nan, inplace=True)\n",
    "\n",
    "### calculate qtd and ytd benchmarks\n",
    "##CREATE A BENCHMARK (based on quarter median)\n",
    "df_amsb_medianq1=df_tdef55.groupby(['metric_name','sm_cost_center'], as_index=False)['q1'].median()\n",
    "df_amsb_medianq2=df_tdef55.groupby(['metric_name','sm_cost_center'], as_index=False)['q2'].median()\n",
    "df_amsb_medianq3=df_tdef55.groupby(['metric_name','sm_cost_center'], as_index=False)['q3'].median()\n",
    "df_amsb_medianq4=df_tdef55.groupby(['metric_name','sm_cost_center'], as_index=False)['q4'].median()\n",
    "df_amsb_medianytd=df_tdef55.groupby(['metric_name','sm_cost_center'], as_index=False)['ytd'].median()\n",
    "##rename median\n",
    "df_amsb_medianq1.rename(columns = {'q1':'q1_benchmark'}, inplace = True)\n",
    "df_amsb_medianq2.rename(columns = {'q2':'q2_benchmark'}, inplace = True)\n",
    "df_amsb_medianq3.rename(columns = {'q3':'q3_benchmark'}, inplace = True)\n",
    "df_amsb_medianq4.rename(columns = {'q4':'q4_benchmark'}, inplace = True)\n",
    "df_amsb_medianytd.rename(columns = {'ytd':'ytd_benchmark'}, inplace = True)\n",
    "\n",
    "### merge to dataframe\n",
    "tdef_pt1=pd.merge(pd.merge(df_tdef55,df_amsb_medianq1,\n",
    "                on=['metric_name','sm_cost_center'],\n",
    "                how='left'),df_amsb_medianq2,on=['metric_name','sm_cost_center'],how='left')\n",
    "\n",
    "tdef_pt2=pd.merge(pd.merge(tdef_pt1,df_amsb_medianq3,\n",
    "                on=['metric_name','sm_cost_center'],\n",
    "                how='left'),df_amsb_medianq4,on=['metric_name','sm_cost_center'],how='left')\n",
    "\n",
    "tdef_pt3=pd.merge(tdef_pt2,df_amsb_medianytd,on=['metric_name','sm_cost_center'],how=\"left\")\n",
    "\n",
    "\n",
    "tdef_pt3['record_date']=rec_date\n",
    "tdef_pt3['level']='AMSB'\n",
    "tdef_pt3['scorecard_filter']='SBB'\n",
    "\n",
    "#tdef_pt3=tdef_pt3.update(tdef_pt3[['november','december','january','february','march','april','may','june','july','august',\n",
    "#                                   'september','october','q1','q1_benchmark','q2','q2_benchmark','q3','q3_benchmark','q4',\n",
    "#                                   'q4_benchmark','ytd','ytd_benchmark']].fillna(0))\n",
    "\n",
    "tdef_amsb=tdef_pt3[['metric_name','level','acf2_id','employee_name',\n",
    "                'november','december','january','february','march','april','may','june','july',\n",
    "                'august','september','october','q1','q1_benchmark','q2','q2_benchmark','q3',\n",
    "               'q3_benchmark','q4','q4_benchmark','ytd','ytd_benchmark',\n",
    "               'am_cost_center','am_cost_center_name','am_cost_center_full_name',\n",
    "               'sm_cost_center','sm_cost_center_name','scorecard_filter','record_date']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# CREATE THE AM OUTPUT ##########################################\n",
    "\n",
    "## aggregate\n",
    "tdef110=amgrpfunc(tdef_amsb)\n",
    "\n",
    "##CREATE THE Q1,Q2,Q3,Q3,YTD VARIABLES\n",
    "newvarfunc(tdef110)\n",
    "\n",
    "## replace 0 with NaN so median only counts non-zeros\n",
    "tdef110.replace(0, np.nan, inplace=True)\n",
    "\n",
    "### calculate qtd and ytd benchmarks\n",
    "##CREATE A BENCHMARK (based on quarter median)\n",
    "df_am_medianq1=tdef110.groupby(['metric_name'], as_index=False)['q1'].median()\n",
    "df_am_medianq2=tdef110.groupby(['metric_name'], as_index=False)['q2'].median()\n",
    "df_am_medianq3=tdef110.groupby(['metric_name'], as_index=False)['q3'].median()\n",
    "df_am_medianq4=tdef110.groupby(['metric_name'], as_index=False)['q4'].median()\n",
    "df_am_medianytd=tdef110.groupby(['metric_name'], as_index=False)['ytd'].median()\n",
    "##rename median\n",
    "df_am_medianq1.rename(columns = {'q1':'q1_benchmark'}, inplace = True)\n",
    "df_am_medianq2.rename(columns = {'q2':'q2_benchmark'}, inplace = True)\n",
    "df_am_medianq3.rename(columns = {'q3':'q3_benchmark'}, inplace = True)\n",
    "df_am_medianq4.rename(columns = {'q4':'q4_benchmark'}, inplace = True)\n",
    "df_am_medianytd.rename(columns = {'ytd':'ytd_benchmark'}, inplace = True)\n",
    "\n",
    "### merge to dataframe\n",
    "tdef_pt11=pd.merge(pd.merge(tdef110,df_am_medianq1,\n",
    "                on=['metric_name'],\n",
    "                how='left'),df_am_medianq2,on=['metric_name'],how='left')\n",
    "\n",
    "tdef_pt22=pd.merge(pd.merge(tdef_pt11,df_am_medianq3,\n",
    "                on=['metric_name'],\n",
    "                how='left'),df_am_medianq4,on=['metric_name'],how='left')\n",
    "\n",
    "tdef_pt338=pd.merge(tdef_pt22,df_am_medianytd,on=['metric_name'],how=\"left\")\n",
    "\n",
    "tdef_pt338['acf2_id']=''\n",
    "tdef_pt338['level']='AM'\n",
    "tdef_pt338['scorecard_filter']='SBB'\n",
    "tdef_pt338['employee_name']=tdef_pt338.am_cost_center_full_name\n",
    "\n",
    "tdef_am_first=tdef_pt338[['metric_name','level','acf2_id','employee_name','november','december','january',\n",
    "           'february','march','april','may','june','july','august','september','october',\n",
    "           'q1','q1_benchmark','q2','q2_benchmark','q3','q3_benchmark','q4','q4_benchmark',\n",
    "           'ytd','ytd_benchmark','am_cost_center','am_cost_center_name','am_cost_center_full_name',\n",
    "               'sm_cost_center','sm_cost_center_name','scorecard_filter','record_date']]\n",
    "\n",
    "## create a copy for Total\n",
    "tdef_am_tot=tdef_am_first.copy()\n",
    "tdef_am_tot['scorecard_filter']='Total'\n",
    "\n",
    "tdef_am_tot1=tdef_am_tot[['metric_name','level','acf2_id','employee_name','november','december','january',\n",
    "           'february','march','april','may','june','july','august','september','october',\n",
    "           'q1','q1_benchmark','q2','q2_benchmark','q3','q3_benchmark','q4','q4_benchmark',\n",
    "           'ytd','ytd_benchmark','am_cost_center','am_cost_center_name','am_cost_center_full_name',\n",
    "               'sm_cost_center','sm_cost_center_name','scorecard_filter','record_date']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# CREATE THE SM OUTPUT ##########################################\n",
    "\n",
    "tdef111=smgrpfunc(tdef_amsb)\n",
    "\n",
    "##CREATE THE Q1,Q2,Q3,Q3,YTD VARIABLES\n",
    "newvarfunc(tdef111)\n",
    "\n",
    "## replace 0 with NaN so median only counts non-zeros\n",
    "tdef111.replace(0, np.nan, inplace=True)\n",
    "\n",
    "### calculate qtd and ytd benchmarks\n",
    "##CREATE A BENCHMARK (based on quarter median)\n",
    "df_sm_medianq1=tdef111.groupby(['metric_name'], as_index=False)['q1'].median()\n",
    "df_sm_medianq2=tdef111.groupby(['metric_name'], as_index=False)['q2'].median()\n",
    "df_sm_medianq3=tdef111.groupby(['metric_name'], as_index=False)['q3'].median()\n",
    "df_sm_medianq4=tdef111.groupby(['metric_name'], as_index=False)['q4'].median()\n",
    "df_sm_medianytd=tdef111.groupby(['metric_name'], as_index=False)['ytd'].median()\n",
    "##rename median\n",
    "df_sm_medianq1.rename(columns = {'q1':'q1_benchmark'}, inplace = True)\n",
    "df_sm_medianq2.rename(columns = {'q2':'q2_benchmark'}, inplace = True)\n",
    "df_sm_medianq3.rename(columns = {'q3':'q3_benchmark'}, inplace = True)\n",
    "df_sm_medianq4.rename(columns = {'q4':'q4_benchmark'}, inplace = True)\n",
    "df_sm_medianytd.rename(columns = {'ytd':'ytd_benchmark'}, inplace = True)\n",
    "\n",
    "### merge to dataframe\n",
    "tdef_pt111=pd.merge(pd.merge(tdef111,df_sm_medianq1,\n",
    "                on=['metric_name'],\n",
    "                how='left'),df_sm_medianq2,on=['metric_name'],how='left')\n",
    "\n",
    "tdef_pt222=pd.merge(pd.merge(tdef_pt111,df_sm_medianq3,\n",
    "                on=['metric_name'],\n",
    "                how='left'),df_sm_medianq4,on=['metric_name'],how='left')\n",
    "\n",
    "tdef_pt30=pd.merge(tdef_pt222,df_am_medianytd,on=['metric_name'],how=\"left\")\n",
    "\n",
    "tdef_pt30['acf2_id']=''\n",
    "tdef_pt30['level']='SM'\n",
    "tdef_pt30['scorecard_filter']='SBB'\n",
    "tdef_pt30['employee_name']=tdef_pt30.sm_cost_center_name\n",
    "tdef_pt30['am_cost_center']=''\n",
    "tdef_pt30['am_cost_center_name']=''\n",
    "tdef_pt30['am_cost_center_full_name']=''\n",
    "\n",
    "tdef_pt350=tdef_pt30[['metric_name','level','acf2_id','employee_name','november','december','january',\n",
    "           'february','march','april','may','june','july','august','september','october',\n",
    "           'q1','q1_benchmark','q2','q2_benchmark','q3','q3_benchmark','q4','q4_benchmark',\n",
    "           'ytd','ytd_benchmark','am_cost_center','am_cost_center_name','am_cost_center_full_name',\n",
    "               'sm_cost_center','sm_cost_center_name','scorecard_filter','record_date']]\n",
    "\n",
    "## create a copy for Total\n",
    "tdef_sm_tot=tdef_pt350.copy()\n",
    "tdef_sm_tot['scorecard_filter']='Total'\n",
    "\n",
    "tdef_sm_tot1=tdef_sm_tot[['metric_name','level','acf2_id','employee_name','november','december','january',\n",
    "           'february','march','april','may','june','july','august','september','october',\n",
    "           'q1','q1_benchmark','q2','q2_benchmark','q3','q3_benchmark','q4','q4_benchmark',\n",
    "           'ytd','ytd_benchmark','am_cost_center','am_cost_center_name','am_cost_center_full_name',\n",
    "               'sm_cost_center','sm_cost_center_name','scorecard_filter','record_date']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdef_sm_tot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## CREATE THE NATIONAL OFFICE OUTPUT ##################################\n",
    "tdefnat=natgrpfunc(tdef_amsb)\n",
    "\n",
    "##CREATE THE Q1,Q2,Q3,Q3,YTD VARIABLES\n",
    "newvarfunc(tdefnat)\n",
    "\n",
    "## replace 0 with NaN so median only counts non-zeros\n",
    "tdefnat.replace(0, np.nan, inplace=True)\n",
    "\n",
    "### calculate qtd and ytd benchmarks\n",
    "##CREATE A BENCHMARK (based on quarter median)\n",
    "df_nso_medianq1=tdefnat.groupby(['metric_name'], as_index=False)['q1'].median()\n",
    "df_nso_medianq2=tdefnat.groupby(['metric_name'], as_index=False)['q2'].median()\n",
    "df_nso_medianq3=tdefnat.groupby(['metric_name'], as_index=False)['q3'].median()\n",
    "df_nso_medianq4=tdefnat.groupby(['metric_name'], as_index=False)['q4'].median()\n",
    "df_nso_medianytd=tdefnat.groupby(['metric_name'], as_index=False)['ytd'].median()\n",
    "##rename median\n",
    "df_nso_medianq1.rename(columns = {'q1':'q1_benchmark'}, inplace = True)\n",
    "df_nso_medianq2.rename(columns = {'q2':'q2_benchmark'}, inplace = True)\n",
    "df_nso_medianq3.rename(columns = {'q3':'q3_benchmark'}, inplace = True)\n",
    "df_nso_medianq4.rename(columns = {'q4':'q4_benchmark'}, inplace = True)\n",
    "df_nso_medianytd.rename(columns = {'ytd':'ytd_benchmark'}, inplace = True)\n",
    "\n",
    "### merge to dataframe\n",
    "tdef_pt111=pd.merge(pd.merge(tdefnat,df_nso_medianq1,\n",
    "                on=['metric_name'],\n",
    "                how='left'),df_nso_medianq2,on=['metric_name'],how='left')\n",
    "\n",
    "tdef_pt222=pd.merge(pd.merge(tdef_pt111,df_nso_medianq3,\n",
    "                on=['metric_name'],\n",
    "                how='left'),df_nso_medianq4,on=['metric_name'],how='left')\n",
    "\n",
    "tdef_nso=pd.merge(tdef_pt222,df_nso_medianytd,on=['metric_name'],how=\"left\")\n",
    "\n",
    "tdef_nso['acf2_id']=''\n",
    "tdef_nso['level']='National Office'\n",
    "tdef_nso['scorecard_filter']='Total'\n",
    "tdef_nso['employee_name']='National Office'\n",
    "tdef_nso['am_cost_center']=''\n",
    "tdef_nso['am_cost_center_name']=''\n",
    "tdef_nso['am_cost_center_full_name']=''\n",
    "tdef_nso['sm_cost_center']=''\n",
    "tdef_nso['sm_cost_center_name']=''\n",
    "tdef_nso['record_date']=rec_date\n",
    "\n",
    "tdef_nso1=tdef_nso[['metric_name','level','acf2_id','employee_name','november','december','january',\n",
    "           'february','march','april','may','june','july','august','september','october',\n",
    "           'q1','q1_benchmark','q2','q2_benchmark','q3','q3_benchmark','q4','q4_benchmark',\n",
    "           'ytd','ytd_benchmark','am_cost_center','am_cost_center_name','am_cost_center_full_name',\n",
    "               'sm_cost_center','sm_cost_center_name','scorecard_filter','record_date']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ CONCATENATE ALL DATAFRAMES ########################################\n",
    "tdef_all=pd.concat([tdef_amsb,tdef_am_first,tdef_am_tot1,tdef_pt350,tdef_sm_tot1,tdef_nso1],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill with zeroes\n",
    "tdef_allf=tdef_all.replace(np.nan,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tdef_allf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#create schema for your dataframe\n",
    "schema = StructType([StructField(\"metric_name\", StringType(), True)\\\n",
    "                   ,StructField(\"level\", StringType(), True)\\\n",
    "                   ,StructField(\"acf2_id\", StringType(), True)\\\n",
    "                     ,StructField(\"employee_name\", StringType(), True)\\\n",
    "                     ,StructField(\"november\", FloatType(), True)\\\n",
    "                     ,StructField(\"december\", FloatType(), True)\\\n",
    "                     ,StructField(\"january\", FloatType(), True)\\\n",
    "                     ,StructField(\"february\", FloatType(), True)\\\n",
    "                     ,StructField(\"march\", FloatType(), True)\\\n",
    "                     ,StructField(\"april\", FloatType(), True)\\\n",
    "                     ,StructField(\"may\", FloatType(), True)\\\n",
    "                     ,StructField(\"june\", FloatType(), True)\\\n",
    "                     ,StructField(\"july\", FloatType(), True)\\\n",
    "                     ,StructField(\"august\", FloatType(), True)\\\n",
    "                     ,StructField(\"september\", FloatType(), True)\\\n",
    "                     ,StructField(\"october\", FloatType(), True)\\\n",
    "                     ,StructField(\"q1\", FloatType(), True)\\\n",
    "                     ,StructField(\"q1_benchmark\", FloatType(), True)\\\n",
    "                     ,StructField(\"q2\", FloatType(), True)\\\n",
    "                     ,StructField(\"q2_benchmark\", FloatType(), True)\\\n",
    "                     ,StructField(\"q3\", FloatType(), True)\\\n",
    "                     ,StructField(\"q3_benchmark\", FloatType(), True)\\\n",
    "                    ,StructField(\"q4\", FloatType(), True)\\\n",
    "                     ,StructField(\"q4_benchmark\", FloatType(), True)\\\n",
    "                     ,StructField(\"ytd\", FloatType(), True)\\\n",
    "                     ,StructField(\"ytd_benchmark\", FloatType(), True)\\\n",
    "                     ,StructField(\"am_cost_center\", StringType(), True)\\\n",
    "                     ,StructField(\"am_cost_center_name\", StringType(), True)\\\n",
    "                     ,StructField(\"am_cost_center_full_name\", StringType(), True)\\\n",
    "                     ,StructField(\"sm_cost_center\", StringType(), True)\\\n",
    "                      ,StructField(\"sm_cost_center_name\", StringType(), True)\\\n",
    "                      ,StructField(\"scorecard_filter\", StringType(), True)\\\n",
    "                      ,StructField(\"record_date\", StringType(), True)])\n",
    "\n",
    "#create spark dataframe using schema\n",
    "tdeffinal = spark.createDataFrame(tdef_allf,schema=schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temp change for F24 roll over\n",
    "#tdeffinal = tdeffinal.toPandas()\n",
    "#cols = ['november','december','january',\n",
    "#           'february','march','april','may','june','july','august','september','october',\n",
    "#           ]\n",
    "#for col in cols:\n",
    "#    tdeffinal[col].values[:] = 0\n",
    "    \n",
    "#tdeffinal.head(5)\n",
    "#tdeffinal=spark.createDataFrame(tdeffinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert pandas dataframe to Pyspark dataframe and save in table\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "tdeffinal.write.mode(\"overwrite\").partitionBy(\"record_date\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_final.SBB_SC_OUTPUT_TDEF\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## END OF PROGRAM"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
