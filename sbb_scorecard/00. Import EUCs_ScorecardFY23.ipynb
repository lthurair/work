{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import HiveContext, SQLContext,SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date,timedelta\n",
    "from datetime import datetime\n",
    "sc = SparkContext(appName = \"Combined_Table\")\n",
    "sql_context = HiveContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "spark = (SparkSession.builder.enableHiveSupport().getOrCreate())\n",
    "\n",
    "\n",
    "#from pyspark.context import SparkContext\n",
    "#from pyspark.sql import HiveContext, SparkSession\n",
    "\n",
    "# Creating a SparkContext is a must\n",
    "#sc = SparkContext(appName=\"profiler\")\n",
    "# Optional creation of a HiveContext\n",
    "#sql_context = HiveContext(sc)\n",
    "# Optional creation of a SparkSession\n",
    "#spark = SparkSession(sc)\n",
    "#spark = (SparkSession.builder.enableHiveSupport().getOrCreate())\n",
    "\n",
    "\n",
    "#from pyspark import SparkConf, SparkContext\n",
    "#from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import *\n",
    "#import subprocess\n",
    "#import sys\n",
    "\n",
    "#appname='aa'\n",
    "#spark = (SparkSession.builder.appName(appname)\n",
    "#         .config(\"hive.metastore.uris\", \"thrift://anp-r09mstr.c03.hadoop.td.com:9083\")\n",
    "#         .config(\"hive.metastore.client.socket.timeout\", \"300\")\n",
    "#         .config(\"hive.metastore.warehouse.dir\", \"/user/hive/warehouse\")\n",
    "#         .config(\"hive.warehouse.subdir.inherit.perms\", \"true\")\n",
    "#         .config(\"hive.execution.engine\", \"spark\")\n",
    "#         .config(\"hive.metastore.execute.setugi\", \"true\")\n",
    "#         .config(\"hive.support.concurrency\", \"true\")\n",
    "#         .config(\"hive.zookeeper.quorum\",\n",
    "#                 \"anp-r04mstr.c03.hadoop.td.com,anp-r02mstr.c03.hadoop.td.com,anp-r01mstr.c03.hadoop.td.com\")\n",
    "#         .config(\"hive.zookeeper.client.port\", \"2181\")\n",
    "#         .config(\"hive.zookeeper.namespace\", \"hive_zookeeper_namespace_hiveHA\")\n",
    "#         .config(\"hive.cluster.delegation.token.store.class\", \"org.apache.hadoop.hive.thrift.DBTokenStore\")\n",
    "#         .config(\"hive.server2.enable.doAs\", \"false\")\n",
    "#         .config(\"hive.metastore.sasl.enabled\", \"true\")\n",
    "#         .config(\"hive.server2.authentication\", \"kerberos\")\n",
    "#         .config(\"hive.metastore.kerberos.principal\", \"hive/_HOST@C03.HADOOP.TD.COM\")\n",
    "#         .config(\"hive.server2.authentication.kerberos.principal\",\n",
    "#                 \"hive/hiveserver.c03.hadoop.td.com@C03.HADOOP.TD.COM\")\n",
    "#         .config(\"hive.server2.use.SSL\", \"true\")\n",
    "#         .config(\"hive.jobexec.dynamic.partition\", \"true\")\n",
    "#         .config(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "#         .config(\"hive.jobexec.dynamic.partition.mode\", \"nonstrict\").enableHiveSupport().getOrCreate())\n",
    "#spark.sparkContext.setLogLevel('INFO')\n",
    "\n",
    "#var1=str(sys.argv[1])\n",
    "#var2=str(sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from numpy import nan\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Joe\n",
    "## read csv\n",
    "datafile=spark.read.csv(\"/tenantspace/ida/anp/cabbtdct1/hivewarehouse/sharb24/CLCP_Dec.csv\",header=True)\n",
    "#datafile.show(5)\n",
    "\n",
    "## overwrite\n",
    "datafile.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_sandbox.CLCP_S680\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_sandbox.CUSTOMER_DECISION_JAN2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read SM csv\n",
    "sm=spark.read.csv(\"/tenantspace/ida/anp/cabbtdct1/hivewarehouse/thural2/SM_HISTORY.csv\",header=True)\n",
    "sm.show(5)\n",
    "\n",
    "## overwrite\n",
    "sm.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.SM_FY22_SCORECARD_HISTORY\")\n",
    "\n",
    "df=spark.sql (\"select * from anp_cabbtdct1_working.SM_FY22_SCORECARD_HISTORY limit 10\")\n",
    "pdf=df.toPandas()\n",
    "pdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read AM csv\n",
    "am=spark.read.csv(\"/tenantspace/ida/anp/cabbtdct1/hivewarehouse/thural2/AM_HISTORY.csv\",header=True)\n",
    "am.show(5)\n",
    "\n",
    "## overwrite\n",
    "am.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.AM_FY22_SCORECARD_HISTORY\")\n",
    "\n",
    "df=spark.sql (\"select * from anp_cabbtdct1_working.AM_FY22_SCORECARD_HISTORY limit 10\")\n",
    "pdf=df.toPandas()\n",
    "pdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LEI csv file (change the tenantspace path name)\n",
    "leif=spark.read.csv(\"/tenantspace/ida/anp/cabbtdct1/hivewarehouse/sharb24/LEI_Modified_Dec2023.csv\",header=True)\n",
    "leif.show(5)\n",
    "\n",
    "## overwrite\n",
    "#leif.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.SBB_LEI\")\n",
    "leif.write.mode(\"append\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.SBB_LEI_F24\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = leif.toPandas()\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import GNB file (change tenantspace path for username)\n",
    "gnbf=spark.read.csv(\"/tenantspace/ida/anp/cabbtdct1/hivewarehouse/sharb24/GNB_Dec2023.csv\",header=True)\n",
    "\n",
    "## overwrite\n",
    "#gnbf.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.SBB_GNB_F24\")\n",
    "gnbf.write.mode(\"append\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.SBB_GNB_F24\")\n",
    "#pdf=gnbf.toPandas()\n",
    "#pdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import TDEF file --not partitioned because it is a cumulative file\n",
    "tdef=spark.read.csv(\"/tenantspace/ida/anp/cabbtdct1/hivewarehouse/sharb24/TDEF.csv\",header=True)\n",
    "#tdef.show(5)\n",
    "\n",
    "## overwrite\n",
    "tdef.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.SBB_TDEF\")\n",
    "pdf=tdef.toPandas()\n",
    "pdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.sql(\"select distinct a.* from anp_cabbtdct1_working.SBB_LEI a inner join (select max(record_date) as max_lei from anp_cabbtdct1_working.SBB_LEI)b on a.record_date=b.max_lei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lei=df1.toPandas()\n",
    "df_lei.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.table('anp_cabbtdct1_working.SBB_LEI')\n",
    "df2=sqlContext.sql(\"select max(record_date) as max_lei from anp_cabbtdct1_working.SBB_LEI\")\n",
    "df2 = spark.table('db.table2')\n",
    "df3 = df1.join(df2, how='inner',on = F.col('record_date') == F.col('max_lei'))\n",
    "df4 = df1.join(df2, how='inner',on='field_name').join(df3,how='left',on='field_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8=spark.sql(\"select * from bbir_vw.hrm_employee_profile limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read FY GOALS csv\n",
    "goal=spark.read.csv(\"/tenantspace/ida/anp/cabbtdct1/hivewarehouse/sharb24/SBB_FY24_GOALS.csv\",header=True)\n",
    "goal.show(5)\n",
    "\n",
    "## overwrite\n",
    "goal.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"anp_cabbtdct1_working.SBB_FY_GOALS\")\n",
    "\n",
    "#df=spark.sql (\"select * from anp_cabbtdct1_working.SBB_FY_GOALS limit 10\")\n",
    "#pdf=df.toPandas()\n",
    "#pdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql (\"select * from anp_cabbtdct1_working.SBB_FY_GOALS\")\n",
    "pdf=df.toPandas()\n",
    "pdf.columns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
